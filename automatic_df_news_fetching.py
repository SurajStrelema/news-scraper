# -*- coding: utf-8 -*-
"""Automatic_DF_News_Fetching.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AQ-z6ZwjFjRIeiKA85HolzGYlQx5BRxz
"""

# news new code with titles

# for fetching news links for multiple keywords

from selenium import webdriver

from selenium.webdriver.common.by import By

from selenium.webdriver.chrome.service import Service

from selenium.webdriver.chrome.options import Options

from webdriver_manager.chrome import ChromeDriverManager

import time

from datetime import datetime, timedelta

import pandas as pd

# Set up Chrome options

options = Options()

options.add_argument("--headless")  # Run in headless mode for faster execution

options.add_argument("--disable-blink-features=AutomationControlled")

options.add_argument("start-maximized")

options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36")

# Initialize WebDriver

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# Define keywords with separate page limits

keywords_with_pages = {
     "Devendra Fadnavis" : 30,
     "देवेंद्र फडणवीस" : 30,
     "CM Devendra Fadnavis" : 30,
     "मुख्यमंत्री देवेंद्र फडणवीस" :30,
     "Maharashtra CM Devendra Fadnavis" : 30,
     "महाराष्ट्र मुख्यमंत्री देवेंद्र फडणवीस" : 30,
    "देवेंद्र फडणवीस इंग्रजी बातम्या" : 30,
    "Devendra Fadnavis English News" : 30,
    "Devendra Fadnavis Latest News" : 30,
    "देवेंद्र फडणवीस बातम्या" : 30
}

# Date range for search

end_date = datetime.today().strftime("%m/%d/%Y")  # Current date

start_date = (datetime.today() - timedelta(days=1)).strftime("%m/%d/%Y")  # 7 days before

print(start_date)

print(end_date)

# Data storage

data = []

# Iterate through keywords and their page limits

for search_query, num_pages in keywords_with_pages.items():

    print(f"Searching for: {search_query} ({num_pages} pages)")

    # Construct Google News Search URL

    news_url = f"https://www.google.co.in/search?q={search_query.replace(' ', '+')}&tbm=nws&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}"

    driver.get(news_url)

    time.sleep(5)  # Allow page to load

    for page in range(1, num_pages + 1):  # Loop through pages

        time.sleep(3)

        # Extract titles and links

        titles = driver.find_elements(By.XPATH, "//div[@class='n0jPhd ynAwRc MBeuO nDgy9d']")

        link_elements = driver.find_elements(By.XPATH, "//a[@jsname='YKoRaf']")

        for title_elem, link_elem in zip(titles, link_elements):

            link = link_elem.get_attribute("href")

            title = title_elem.text.strip()

            if link and "google.com" not in link:  # Avoid Google internal links

                data.append({

                    "Keyword": search_query,

                    "Page": page,

                    "Title": title,

                    "Links": link

                })

        # Click 'Next' to go to the next page

        try:

            next_button = driver.find_element(By.XPATH, "//a[@id='pnnext']")

            next_url = next_button.get_attribute("href")

            if next_url:

                driver.get(next_url)

            else:

                print(f"No more pages found for '{search_query}' after {page} pages.")

                break

        except:

            print(f"No 'Next' button found for '{search_query}' after {page} pages.")

            break

# Convert data to DataFrame and save to Excel

df = pd.DataFrame(data, columns=["Keyword", "Page", "Title", "Links"])

output_filename = "C:/Users/HP/Desktop/DF_News/19th_June_news_links.xlsx"

df.to_excel(output_filename, index=False)

print(f"Total links extracted: {len(df)}")

print(f"Data saved to '{output_filename}'")

# Close browser

driver.quit()

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime, timedelta
import pandas as pd
import logging
import os
import random
import time
print("Libraries Loaded")

# Configure logging
logging.basicConfig(
    filename='news_scraper.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logging.info("Starting news scraping script")

# User-agents for rotation
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0"
]

# Keywords and page limits
KEYWORDS_WITH_PAGES = {
    "Devendra Fadnavis": 30,
    "देवेंद्र फडणवीस": 30,
    "CM Devendra Fadnavis": 30,
    "मुख्यमंत्री देवेंद्र फडणवीस": 30,
    "Maharashtra CM Devendra Fadnavis": 30,
    "महाराष्ट्र मुख्यमंत्री देवेंद्र फडणवीस": 30,
    "देवेंद्र फडणवीस इंग्रजी बातम्या": 30,
    "Devendra Fadnavis English News": 30,
    "Devendra Fadnavis Latest News": 30,
    "देवेंद्र फडणवीस बातम्या": 30
}

# Output directory and filename
OUTPUT_DIR = "C:/Users/HP/Desktop/DF_News/"
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_FILENAME = os.path.join(OUTPUT_DIR, f"news_links_{TIMESTAMP}.xlsx")

def setup_driver():
    """Initialize and configure Chrome WebDriver."""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument(f"user-agent={random.choice(USER_AGENTS)}")
    options.add_argument("start-maximized")

    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        logging.info("WebDriver initialized successfully")
        return driver
    except Exception as e:
        logging.error(f"Failed to initialize WebDriver: {e}")
        raise

def scrape_page(driver, keyword, page_num, seen_links):
    """Scrape news titles and links from a single page."""
    data = []
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div[role='heading']"))
        )

        titles = driver.find_elements(By.XPATH, "//div[@class='n0jPhd ynAwRc MBeuO nDgy9d']")

        link_elements = driver.find_elements(By.XPATH, "//a[@jsname='YKoRaf']")

        for title_elem, link_elem in zip(titles, link_elements):

            link = link_elem.get_attribute("href")

            title = title_elem.text.strip()

            if not link or "google.com" in link or link in seen_links:
                continue

            seen_links.add(link)
            data.append({
                "Keyword": keyword,
                "Page": page_num,
                "Title": title,
                "Links": link
            })

        logging.info(f"Scraped {len(data)} articles from page {page_num} for '{keyword}'")
        return data

    except Exception as e:
        logging.warning(f"Error scraping page {page_num} for '{keyword}': {e}")
        return data

def scrape_keyword(driver, keyword, num_pages, start_date, end_date, seen_links):
    """Scrape all pages for a given keyword."""
    data = []
    search_query = keyword.replace(" ", "+")
    news_url = (
        f"https://www.google.co.in/search?q={search_query}&tbm=nws"
        f"&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}"
    )

    try:
        driver.get(news_url)
        logging.info(f"Starting scrape for '{keyword}' at {news_url}")

        for page in range(1, num_pages + 1):
            page_data = scrape_page(driver, keyword, page, seen_links)
            data.extend(page_data)

            if page < num_pages:
                try:
                    next_button = WebDriverWait(driver, 5).until(
                        EC.element_to_be_clickable((By.ID, "pnnext"))
                    )
                    next_url = next_button.get_attribute("href")
                    if not next_url:
                        logging.info(f"No more pages for '{keyword}' after page {page}")
                        break
                    driver.get(next_url)
                except Exception as e:
                    logging.info(f"No 'Next' button found for '{keyword}' after page {page}: {e}")
                    break

    except Exception as e:
        logging.error(f"Error processing keyword '{keyword}': {e}")

    return data

def save_data(data, output_filename):
    """Save scraped data to an Excel file."""
    try:
        os.makedirs(os.path.dirname(output_filename), exist_ok=True)
        df = pd.DataFrame(data, columns=["Keyword", "Page", "Title", "Links"])
        df.to_excel(output_filename, index=False)
        logging.info(f"Saved {len(df)} links to '{output_filename}'")
    except Exception as e:
        logging.error(f"Error saving data to '{output_filename}': {e}")
        raise

def main():
    """Main function to orchestrate news scraping."""
    end_date = datetime.today().strftime("%m/%d/%Y")
    start_date = (datetime.today() - timedelta(days=1)).strftime("%m/%d/%Y")
    logging.info(f"Date range: {start_date} to {end_date}")

    all_data = []
    seen_links = set()

    driver = setup_driver()

    try:
        for keyword, num_pages in KEYWORDS_WITH_PAGES.items():
            logging.info(f"Processing keyword: '{keyword}' ({num_pages} pages)")
            keyword_data = scrape_keyword(driver, keyword, num_pages, start_date, end_date, seen_links)
            all_data.extend(keyword_data)

        save_data(all_data, OUTPUT_FILENAME)
        logging.info(f"Total links extracted: {len(all_data)}")

    finally:
        driver.quit()
        logging.info("WebDriver closed")

if __name__ == "__main__":
    main()

